{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7480ad5e-62cd-4762-8a54-faf9ef11684a",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "In this section, we will delve into the practical implementation of Word2Vec using the Gensim library. Here, we'll explore various operations that can be performed on words and words embeddings learned by the model. Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654cf469-2fd4-47c1-a101-bde2fd6b0698",
   "metadata": {},
   "source": [
    "### 2. Setting Up The Environment \n",
    "First, we need to set up our environment. This involves importing necessary libraries and ensuring Gensim is installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d309b8d-dda6-478c-af8c-95588106618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet gensim\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.downloader import load\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c289c9dc-5e31-4bd3-9acb-6cff3e58d0ff",
   "metadata": {},
   "source": [
    "### 3. Loading Word2Vec Vectors into Model\n",
    "\n",
    "I will be using pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "078ea7b2-9bdb-4772-be57-5d32bfe2162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = load('word2vec-google-news-300')\n",
    "\n",
    "# Initialize Word2Vec models\n",
    "w2v_model = Word2Vec(vector_size=wv.vector_size, min_count=1)\n",
    "\n",
    "# Build the vocabulary from the pretrained model\n",
    "w2v_model.build_vocab_from_freq(word_freq={word: wv.get_vecattr(word, 'count') for word in wv.index_to_key})\n",
    "\n",
    "# Inject the pretrained vectors\n",
    "w2v_model.wv.vectors[:] = wv.vectors[:]\n",
    "\n",
    "# Check if the vectors have been injected correctly\n",
    "assert w2v_model.wv['king'].all() == wv['king'].all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b8372f-a4b7-4e10-b170-9f61024abed7",
   "metadata": {},
   "source": [
    "### 4. Exploring the Word Vector Operations\n",
    "Let's explore the word vectors and see how they perform on various tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d17ab8-f0cf-4694-aed7-356ea466705a",
   "metadata": {},
   "source": [
    "#### 4.1. Vector Arithmetics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97383fc8-98d1-4cc8-b05d-2366ad73a584",
   "metadata": {},
   "source": [
    "Imagine you're navigating a map where cities and countries are represented as points. The distance and direction between \"Germany\" and \"Berlin\" on this map captures the relationship of a country to its capital.\n",
    "\n",
    "When you move from \"Germany\" to \"Berlin\" on this map, you're essentially following a \"capital city direction\". Now, if you start at \"France\" and move in the same \"capital city direction\", you should end up at \"Paris\".\n",
    "\n",
    "In the vector space of word embeddings:\n",
    "\n",
    "Moving from \"Germany\" to \"Berlin\" is like subtracting the vector of \"Germany\" from \"Berlin\".\n",
    "To find the equivalent capital for \"France\", you add the \"capital city direction\" to \"France\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40c45374-f97e-4a1a-92ee-5663c64b396e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France - Germany + Berlin = Paris  ---  with similarity score of 0.7672389149665833\n"
     ]
    }
   ],
   "source": [
    "result = w2v_model.wv.most_similar(positive=['France', 'Berlin'], negative=['Germany'], topn=10)\n",
    "print(f\"France - Germany + Berlin = {result[0][0]}  ---  with similarity score of {result[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4458d6-02bc-4f4e-8463-d61d6c1aefb3",
   "metadata": {},
   "source": [
    "#### 4.2. Cosine Similarity\n",
    "We can compute the cosine similarity between two word vectors to measure their semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6249f54-a80f-4919-ab74-6070cc8baf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'boy' and 'girl': 0.8543272018432617\n",
      "Cosine similarity between 'coffee' and 'giraffe': 0.13183525204658508\n"
     ]
    }
   ],
   "source": [
    "similarity = w2v_model.wv.similarity('boy', 'girl')\n",
    "print(f\"Cosine similarity between 'boy' and 'girl': {similarity}\")\n",
    "\n",
    "similarity = w2v_model.wv.similarity('coffee', 'giraffe')\n",
    "print(f\"Cosine similarity between 'coffee' and 'giraffe': {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e26a7d1-4998-4439-9b40-63b9271bfad9",
   "metadata": {},
   "source": [
    "1. **Cosine similarity between 'boy' and 'girl'**: <br>\n",
    "This measures how similar the word vectors for 'boy' and 'girl' are. Given that both words are human genders and are often used in similar contexts, their vectors are likely to be close in the vector space. Therefore, the cosine similarity between them would be a high value, close to 1.\n",
    "\n",
    "2. **Cosine similarity between 'coffee' and 'giraffe'**: <br>\n",
    "Here, we're comparing the word vectors for 'coffee' (a beverage) and 'giraffe' (an animal). Intuitively, these words are not related and are used in very different contexts. As a result, their vectors would be farther apart in the vector space. The cosine similarity between them would be closer to 0, indicating they are not similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeb11c6-f3b6-4e5f-9ade-af513fcf6934",
   "metadata": {},
   "source": [
    "#### 4.3. Word Analogies\n",
    "Word2Vec can solve analogies. For example, \"man\" is to \"king\" as \"woman\" is to \"____?\".\n",
    "\n",
    "Imagine a theater where actors play different roles. In one play, a man plays the role of a king. In another play, if we want a woman to play a similar authoritative role, what would that role be? The answer could be a queen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7712736-6520-4d39-8609-498ae2583982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man:king as woman:queen\n"
     ]
    }
   ],
   "source": [
    "# This is similar to vector arithmetics, but let's try another example\n",
    "result = w2v_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(f\"man:king as woman:{result[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e66157a-ee3a-4521-8f8f-cdd1bb38cc62",
   "metadata": {},
   "source": [
    "#### 4.4. Find Synonyms\n",
    "We can identify semantically similar or opposite words based on proximity in vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd1f78da-9a7e-4ead-be8e-8048530974f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms for 'car': ['vehicle', 'cars', 'SUV', 'minivan', 'truck']\n"
     ]
    }
   ],
   "source": [
    "synonyms = w2v_model.wv.most_similar('car', topn=5)\n",
    "print(f\"Synonyms for 'car': {[word[0] for word in synonyms]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c331522-cc9b-4776-8c1a-6580934902e2",
   "metadata": {},
   "source": [
    "#### 4.5. Out-of-Vocabulary Words\n",
    "Word2Vec can't directly generate embeddings for words not in the original vocabulary. However, FastText, an extension of Word2Vec, can handle this by averaging subword embeddings.\n",
    "\n",
    "For more information: https://github.com/facebookresearch/fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8f5e6ef-460e-4532-8431-e398033bf677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "fasttext_model = FastText(vector_size=wv.vector_size, min_count=1)\n",
    "\n",
    "# Build the vocabulary from the pretrained model\n",
    "fasttext_model.build_vocab_from_freq(word_freq={word: wv.get_vecattr(word, 'count') for word in wv.index_to_key})\n",
    "\n",
    "# Inject the pretrained vectors\n",
    "fasttext_model.wv.vectors[:] = wv.vectors[:]\n",
    "\n",
    "# Check if the vectors have been injected correctly\n",
    "assert fasttext_model.wv['king'].all() == wv['king'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b00e5da-45df-4463-b533-083446dc6ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print('madeupword' in fasttext_model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acd98b98-8487-443b-b455-c3d92b8e8ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.9911410e-05,  3.8663266e-04,  4.9756694e-04, ...,\n",
       "        3.2902186e-04, -4.7884602e-04, -7.4080104e-05], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv['madeupword']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cfa298-582a-4d10-9b24-a7fbf3e529c9",
   "metadata": {},
   "source": [
    "#### 4.6. Sentence/Document Embeddings\n",
    "To represent entire sentences or documents, we can average the word embeddings of all words in the sentence/document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de5349ab-ed66-46a2-92b0-9995af46112d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for the sentence: [ 0.04085432  0.07670375  0.00636509 ...  0.11017717 -0.04701451\n",
      " -0.0227356 ]\n"
     ]
    }
   ],
   "source": [
    "def sentence_vector(sentence, model):\n",
    "    words = [word for word in sentence.split() if word in model.wv.key_to_index]\n",
    "    if len(words) == 0:\n",
    "        return None\n",
    "    return sum(model.wv[word] for word in words) / len(words)\n",
    "\n",
    "sentence = \"A quick cat is running outside the house.\"\n",
    "vector = sentence_vector(sentence, w2v_model)\n",
    "print(f\"Vector for the sentence: {vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007a4b8d-bc14-4e1e-9c06-82f1cbe3af17",
   "metadata": {},
   "source": [
    "#### 4.7. Semantic Clustering\n",
    "Grouping semantically related words based on their embeddings can be achieved using clustering techniques like K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cb242ac-cd98-4ba6-b84a-9b2f56361575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1: </s>, in, for, that, is, on, ##, The, with, said ...\n",
      "Cluster 2: pm, â€¢, ##:##, *, WASHINGTON, NEW_YORK, Former, #####, Call, vs. ...\n",
      "Cluster 3: %, system, available, products, data, technology, rate, study, product, provides ...\n",
      "Cluster 4: game, team, season, #-#, play, points, win, games, ##-##, players ...\n",
      "Cluster 5: He, her, she, school, She, manager, Smith, district, St., wife ...\n",
      "Cluster 6: India, Mr, Pakistan, Government, Indian, minister, Rs, Secretary, Minister, army ...\n",
      "Cluster 7: company, market, quarter, billion, companies, sales, financial, industry, growth, Inc. ...\n",
      "Cluster 8: government, AP, police, tax, economic, Department, prices, federal, political, military ...\n",
      "Cluster 9: By, ###-####, looking_statements, ###-###-####, Writer, AP_Photo, differ_materially, Tickets, please_visit, Contact ...\n",
      "Cluster 10: I, you, like, your, my, And, me, 've, You, really ...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Extract word vectors and their corresponding words\n",
    "word_vectors = [w2v_model.wv[word] for word in w2v_model.wv.key_to_index]\n",
    "words = list(w2v_model.wv.key_to_index.keys())\n",
    "\n",
    "# Use KMeans to cluster these word vectors into 10 clusters\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(word_vectors)\n",
    "\n",
    "# Display words in each cluster\n",
    "for i in range(10):\n",
    "    cluster_words = [words[j] for j, cluster_id in enumerate(kmeans.labels_) if cluster_id == i]\n",
    "    print(f\"Cluster {i + 1}: {', '.join(cluster_words[:10])} ...\")  # Displaying only the first 10 words for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af35932b-7f72-468d-99f8-0f9d32b90ce9",
   "metadata": {},
   "source": [
    "### 6. References and Further Reading\n",
    "- [Gensim Documentation](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "- [Gensim Source Code](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py)\n",
    "- [Original Word2Vec Paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "- [FastText GitHub Repo](https://github.com/facebookresearch/fastText)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analysis",
   "language": "python",
   "name": "data-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
